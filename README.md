# SCARLET - Web Crawler

## üîç Descripci√≥n
SCARLET es un web crawler as√≠ncrono dise√±ado para descubrir URLs dentro de un mismo dominio. Utiliza `asyncio` y `aiohttp` para realizar peticiones concurrentes de manera eficiente.

## ‚ú® Caracter√≠sticas

- ‚úÖ **Crawling as√≠ncrono**: Utiliza async/await para m√°xima eficiencia
- ‚úÖ **Control de concurrencia**: Sem√°foros para limitar peticiones simult√°neas
- ‚úÖ **Detecci√≥n de subdominios**: Identifica y marca URLs de subdominios
- ‚úÖ **Manejo robusto de errores**: Timeout, reintentos y validaci√≥n de URLs
- ‚úÖ **Estad√≠sticas detalladas**: Muestra m√©tricas al finalizar el proceso
- ‚úÖ **Exportaci√≥n a JSON**: Guarda resultados en formato estructurado
- ‚úÖ **Argumentos CLI**: Configuraci√≥n flexible desde l√≠nea de comandos

## üìã Requisitos

```bash
pip install aiohttp beautifulsoup4
```

## üöÄ Uso

### Uso b√°sico (interactivo)
```bash
python scarlet_v2.py
```
El programa te pedir√° ingresar la URL.

### Con URL directa
```bash
python scarlet_v2.py --url https://ejemplo.com
```

### Con l√≠mite de URLs
```bash
python scarlet_v2.py -u https://ejemplo.com --max-urls 100
```

### Configurar workers concurrentes
```bash
python scarlet_v2.py -u https://ejemplo.com --workers 20
```

### Guardar resultados en JSON
```bash
python scarlet_v2.py -u https://ejemplo.com --output resultados.json
```

### Combinando opciones
```bash
python scarlet_v2.py -u https://ejemplo.com -m 500 -w 15 -o sitio.json
```

### Con sitios que tienen problemas de certificado SSL
```bash
python scarlet_v2.py -u https://sitio-con-ssl-invalido.com -k
```
‚ö†Ô∏è **Advertencia**: Solo usa `-k` para testing. No es recomendado en producci√≥n.

## üîß Par√°metros

| Par√°metro | Abreviatura | Descripci√≥n | Por defecto |
|-----------|-------------|-------------|-------------|
| `--url` | `-u` | URL base a crawlear | Solicitar interactivamente |
| `--max-urls` | `-m` | L√≠mite m√°ximo de URLs a procesar | Sin l√≠mite |
| `--workers` | `-w` | N√∫mero de workers concurrentes | 10 |
| `--output` | `-o` | Archivo de salida JSON | No guardar |
| `--insecure` | `-k` | Ignorar errores SSL (solo testing) | Verificar SSL |

## üìä Salida

El programa muestra en tiempo real:
- URLs encontradas (en amarillo para mismo dominio, azul para subdominios)
- Estado de cada URL (‚úî accesible, X error)
- Estad√≠sticas finales:
  - Total de URLs encontradas
  - URLs procesadas
  - Errores encontrados
  - Tiempo total de ejecuci√≥n

## üìÅ Formato de salida JSON

```json
{
  "timestamp": "2025-11-04T10:30:00",
  "total_urls": 150,
  "urls": [
    "https://ejemplo.com",
    "https://ejemplo.com/about",
    "https://ejemplo.com/contact"
  ]
}
```

## üé® C√≥digos de color

- üü¢ Verde: URL accesible correctamente
- üî¥ Rojo: Error al acceder a la URL
- üü° Amarillo: URL del mismo dominio
- üîµ Azul: URL de subdominio

## ‚öôÔ∏è Mejoras implementadas

1. **Validaci√≥n de URLs**: Verifica formato antes de iniciar
2. **Timeout configurable**: Evita peticiones colgadas (10 segundos)
3. **Sem√°foros para concurrencia**: Control real de peticiones simult√°neas
4. **Manejo de localhost**: Soporte para dominios sin TLD
5. **Eliminaci√≥n de fragmentos**: Evita duplicados por anchors (#)
6. **Estad√≠sticas en tiempo real**: Monitoreo del proceso
7. **Exportaci√≥n de datos**: Resultados en formato JSON
8. **CLI con argparse**: Configuraci√≥n flexible
9. **Docstrings completos**: Documentaci√≥n en cada funci√≥n
10. **Banner optimizado**: Sin delay innecesario

## üêõ Soluci√≥n de problemas

### Error de importaci√≥n
```bash
pip install --upgrade aiohttp beautifulsoup4
```

### Error de certificado SSL
Si encuentras errores como `SSL: CERTIFICATE_VERIFY_FAILED`, puedes usar:
```bash
python scarlet_v2.py -u https://sitio.com -k
```
‚ö†Ô∏è **Nota**: Esto desactiva la verificaci√≥n SSL. √ösalo solo para testing o sitios confiables.

### Demasiados errores de timeout
Reduce el n√∫mero de workers:
```bash
python scarlet_v2.py --workers 5
```

### Proceso muy lento
Aumenta el n√∫mero de workers (con precauci√≥n):
```bash
python scarlet_v2.py --workers 20
```

## üìù Notas

- El crawler respeta el mismo dominio base para evitar salir del sitio objetivo
- Los fragmentos de URL (#section) son ignorados para evitar duplicados
- Solo se procesan p√°ginas HTML, otros tipos de contenido son omitidos
- El timeout por defecto es de 10 segundos por petici√≥n

## üë§ Autor

**3SC0B0T**

---

‚ö° Desarrollado con Python + AsyncIO + aiohttp

